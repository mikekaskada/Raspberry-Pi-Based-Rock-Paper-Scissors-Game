## Ανάλυση της Στρατηγικής με την οποία παίζει ο υπολογιστής
### 1. **Advanced DQN Agent**

Ο υπολογιστής χρησιμοποιεί έναν προηγμένο πράκτορα (agent) Deep Q-Network (DQN) για να καθορίσει τις κινήσεις του. Κύρια χαρακτηριστικά αυτού του πράκτορα περιλαμβάνουν:

- **State Representation:** Το state vector αποτελείται από διάφορα στοιχεία, όπως one-hot encoding των τρεχουσών κινήσεων (ανθρώπου και υπολογιστή), συχνότητες κινήσεων του αντιπάλου, πρόσφατες κινήσεις παίκτη και υπολογιστή, μετρήσεις σειρών και κατηγοριοποίηση αντιπάλου.
- **Epsilon-Greedy Policy:** Ο πράκτορας χρησιμοποιεί μια epsilon-greedy policy για να ισορροπήσει την εξερεύνηση και την εκμετάλλευση, ρυθμίζοντας την τιμή epsilon βάσει του performance metric και των σειρών νικών/ήττων.
- **Replay Memory:** Ο πράκτορας χρησιμοποιεί ένα replay memory για να αποθηκεύει παρελθούσες εμπειρίες και να μαθαίνει από αυτές με τη δειγματοληψία τυχαίων mini-batches κατά την εκπαίδευση.
- **Dynamic Learning Rate:** Ο ρυθμός μάθησης προσαρμόζεται δυναμικά βάσει της απόδοσης του πράκτορα, αυξανόμενος αν η απόδοση είναι καλή και μειούμενος αλλιώς.

### 2. **Ensemble Model**

Η στρατηγική χρησιμοποιεί ένα ensemble model που συνδυάζει ένα DQN και ένα LSTM δίκτυο:

- **DQNetwork:** Ένα πλήρως συνδεδεμένο νευρωνικό δίκτυο που προβλέπει Q-values για διαφορετικές ενέργειες, βοηθώντας τον πράκτορα να επιλέξει την καλύτερη ενέργεια.
- **LSTMNetwork:** Ένα αναδρομικό νευρωνικό δίκτυο (LSTM) για την ανίχνευση μοτίβων σε ακολουθίες κινήσεων, επιτρέποντας στον πράκτορα να προβλέπει την επόμενη κίνηση του αντιπάλου βάσει ιστορικών δεδομένων.
- **EnsembleModel:** Συνδυάζει τις εξόδους των DQN και LSTM δικτύων για να κάνει πιο ανθεκτικές προβλέψεις.

### 3. **Opponent Classification and Clustering**

Ο πράκτορας κατηγοριοποιεί τον αντίπαλο σε μία από τις τρεις κατηγορίες: aggressive, defensive ή random, βάσει των μοτίβων κινήσεών του. Αυτή η κατηγοριοποίηση βοηθά τον πράκτορα να προσαρμόσει ανάλογα τη στρατηγική του. Επιπλέον, ο πράκτορας ομαδοποιεί τις ιστορικές στρατηγικές των αντιπάλων χρησιμοποιώντας KMeans clustering για να αναγνωρίσει και να προσαρμοστεί σε διαφορετικά στυλ παιχνιδιού.

### 4. **Real-Time Adaptation and Learning**

Ο πράκτορας προσαρμόζει συνεχώς τη στρατηγική του βάσει των πραγματικών δεδομένων:

- **Adaptive Strategy:** Ο πράκτορας εναλλάσσει δυναμικά τη στρατηγική του βάσει του performance metric, εναλλάσσοντας μεταξύ διαφορετικών στρατηγικών για να βελτιστοποιήσει την απόδοσή του.
- **Real-Time Learning:** Ο πράκτορας ενημερώνει τα Q-values και τις παραμέτρους του μοντέλου σε πραγματικό χρόνο βάσει των αποτελεσμάτων κάθε γύρου, βελτιώνοντας τη στρατηγική του καθώς προχωρά το παιχνίδι.
- **Pattern Detection:** Το LSTM συστατικό προσπαθεί να ανιχνεύσει επαναλαμβανόμενα μοτίβα στις κινήσεις του αντιπάλου, τα οποία στη συνέχεια χρησιμοποιούνται για την πρόβλεψη μελλοντικών κινήσεων.

### 5. **Pre-Training and Historical Data**

Ο πράκτορας εκπαιδεύεται προκαταβολικά χρησιμοποιώντας ιστορικά δεδομένα παιχνιδιού για να προσφέρει μια ισχυρή αρχική απόδοση. Αυτά τα ιστορικά δεδομένα επεξεργάζονται για να εξάγουν συχνότητες κινήσεων και μετρήσεις μεταβάσεων, οι οποίες χρησιμοποιούνται για την αρχική διαμόρφωση της βάσης γνώσεων του πράκτορα.

### 6. **Reward Mechanism**

Ο μηχανισμός ανταμοιβής είναι απλός, με ανταμοιβές να αποδίδονται βάσει του αποτελέσματος κάθε γύρου:
- **Win:** Θετική ανταμοιβή.
- **Loss:** Αρνητική ανταμοιβή.
- **Tie:** Μηδενική ανταμοιβή.

### 7. **Streak-Based Epsilon Adjustment**

Ο πράκτορας προσαρμόζει το ποσοστό εξερεύνησης (epsilon) βάσει των σειρών νικών και ηττών. Μια σειρά νικών οδηγεί σε περισσότερη εκμετάλλευση (χαμηλότερο epsilon), ενώ μια σειρά ηττών ενθαρρύνει περισσότερη εξερεύνηση (υψηλότερο epsilon).

### 8. **Game History and Save Mechanism**

Η ιστορία του παιχνιδιού, συμπεριλαμβανομένων των κινήσεων και των αποτελεσμάτων κάθε γύρου, αποθηκεύεται για περαιτέρω ανάλυση και μελλοντική εκπαίδευση. Ο πράκτορας αποθηκεύει επίσης περιοδικά το μοντέλο του για να διατηρήσει τις βελτιώσεις με την πάροδο του χρόνου.

### Περιληπτικά

Στην ουσία, η στρατηγική του υπολογιστή σε αυτό το παιχνίδι Rock-Paper-Scissors είναι ένα εξελιγμένο μείγμα τεχνικών μηχανικής μάθησης, που περιλαμβάνει βαθιά ενισχυτική μάθηση, ανίχνευση μοτίβων με LSTM, κατηγοριοποίηση αντιπάλου και δυναμικές στρατηγικές σε πραγματικό χρόνο. Αυτή η πολύπλευρη προσέγγιση επιτρέπει στον υπολογιστή να μαθαίνει και να βελτιώνει συνεχώς την απόδοσή του, καθιστώντας τον έναν δυναμικό αντίπαλο στο παιχνίδι.
