## Ανάλυση της Στρατηγικής με την οποία παίζει ο υπολογιστής
### 1. **Advanced DQN Agent**

Ο υπολογιστής χρησιμοποιεί έναν προηγμένο πράκτορα (agent) Deep Q-Network (DQN) για να καθορίσει τις κινήσεις του. Κύρια χαρακτηριστικά αυτού του πράκτορα περιλαμβάνουν:

- **State Representation:** Το state vector αποτελείται από διάφορα στοιχεία, όπως one-hot encoding των τρεχουσών κινήσεων (ανθρώπου και υπολογιστή), συχνότητες κινήσεων του αντιπάλου, πρόσφατες κινήσεις παίκτη και υπολογιστή, μετρήσεις σειρών και κατηγοριοποίηση αντιπάλου.
- **Epsilon-Greedy Policy:** Ο πράκτορας χρησιμοποιεί μια epsilon-greedy policy για να ισορροπήσει την εξερεύνηση και την εκμετάλλευση, ρυθμίζοντας την τιμή epsilon βάσει του performance metric και των σειρών νικών/ήττων.
- **Replay Memory:** Ο πράκτορας χρησιμοποιεί ένα replay memory για να αποθηκεύει παρελθούσες εμπειρίες και να μαθαίνει από αυτές με τη δειγματοληψία τυχαίων mini-batches κατά την εκπαίδευση.
- **Dynamic Learning Rate:** Ο ρυθμός μάθησης προσαρμόζεται δυναμικά βάσει της απόδοσης του πράκτορα, αυξανόμενος αν η απόδοση είναι καλή και μειούμενος αλλιώς.

### 2. **Ensemble Model**

Η στρατηγική χρησιμοποιεί ένα ensemble model που συνδυάζει ένα DQN και ένα LSTM δίκτυο:

- **DQNetwork:** Ένα πλήρως συνδεδεμένο νευρωνικό δίκτυο που προβλέπει Q-values για διαφορετικές ενέργειες, βοηθώντας τον πράκτορα να επιλέξει την καλύτερη ενέργεια.
- **LSTMNetwork:** Ένα αναδρομικό νευρωνικό δίκτυο (LSTM) για την ανίχνευση μοτίβων σε ακολουθίες κινήσεων, επιτρέποντας στον πράκτορα να προβλέπει την επόμενη κίνηση του αντιπάλου βάσει ιστορικών δεδομένων.
- **EnsembleModel:** Συνδυάζει τις εξόδους των DQN και LSTM δικτύων για να κάνει πιο ανθεκτικές προβλέψεις.

### 3. **Opponent Classification and Clustering**

Ο πράκτορας κατηγοριοποιεί τον αντίπαλο σε μία από τις τρεις κατηγορίες: aggressive, defensive ή random, βάσει των μοτίβων κινήσεών του. Αυτή η κατηγοριοποίηση βοηθά τον πράκτορα να προσαρμόσει ανάλογα τη στρατηγική του. Επιπλέον, ο πράκτορας ομαδοποιεί τις ιστορικές στρατηγικές των αντιπάλων χρησιμοποιώντας KMeans clustering για να αναγνωρίσει και να προσαρμοστεί σε διαφορετικά στυλ παιχνιδιού.

### 4. **Real-Time Adaptation and Learning**

Ο πράκτορας προσαρμόζει συνεχώς τη στρατηγική του βάσει των πραγματικών δεδομένων:

- **Adaptive Strategy:** Ο πράκτορας εναλλάσσει δυναμικά τη στρατηγική του βάσει του performance metric, εναλλάσσοντας μεταξύ διαφορετικών στρατηγικών για να βελτιστοποιήσει την απόδοσή του.
- **Real-Time Learning:** Ο πράκτορας ενημερώνει τα Q-values και τις παραμέτρους του μοντέλου σε πραγματικό χρόνο βάσει των αποτελεσμάτων κάθε γύρου, βελτιώνοντας τη στρατηγική του καθώς προχωρά το παιχνίδι.
- **Pattern Detection:** Το LSTM συστατικό προσπαθεί να ανιχνεύσει επαναλαμβανόμενα μοτίβα στις κινήσεις του αντιπάλου, τα οποία στη συνέχεια χρησιμοποιούνται για την πρόβλεψη μελλοντικών κινήσεων.

### 5. **Pre-Training and Historical Data**

Ο πράκτορας εκπαιδεύεται προκαταβολικά χρησιμοποιώντας ιστορικά δεδομένα παιχνιδιού για να προσφέρει μια ισχυρή αρχική απόδοση. Αυτά τα ιστορικά δεδομένα επεξεργάζονται για να εξάγουν συχνότητες κινήσεων και μετρήσεις μεταβάσεων, οι οποίες χρησιμοποιούνται για την αρχική διαμόρφωση της βάσης γνώσεων του πράκτορα.

### 6. **Reward Mechanism**

Ο μηχανισμός ανταμοιβής είναι απλός, με ανταμοιβές να αποδίδονται βάσει του αποτελέσματος κάθε γύρου:
- **Win:** Θετική ανταμοιβή.
- **Loss:** Αρνητική ανταμοιβή.
- **Tie:** Μηδενική ανταμοιβή.

### 7. **Streak-Based Epsilon Adjustment**

Ο πράκτορας προσαρμόζει το ποσοστό εξερεύνησης (epsilon) βάσει των σειρών νικών και ηττών. Μια σειρά νικών οδηγεί σε περισσότερη εκμετάλλευση (χαμηλότερο epsilon), ενώ μια σειρά ηττών ενθαρρύνει περισσότερη εξερεύνηση (υψηλότερο epsilon).

### 8. **Game History and Save Mechanism**

Η ιστορία του παιχνιδιού, συμπεριλαμβανομένων των κινήσεων και των αποτελεσμάτων κάθε γύρου, αποθηκεύεται για περαιτέρω ανάλυση και μελλοντική εκπαίδευση. Ο πράκτορας αποθηκεύει επίσης περιοδικά το μοντέλο του για να διατηρήσει τις βελτιώσεις με την πάροδο του χρόνου.

### Περιληπτικά

Στην ουσία, η στρατηγική του υπολογιστή σε αυτό το παιχνίδι Rock-Paper-Scissors είναι ένα εξελιγμένο μείγμα τεχνικών μηχανικής μάθησης, που περιλαμβάνει βαθιά ενισχυτική μάθηση, ανίχνευση μοτίβων με LSTM, κατηγοριοποίηση αντιπάλου και δυναμικές στρατηγικές σε πραγματικό χρόνο. Αυτή η πολύπλευρη προσέγγιση επιτρέπει στον υπολογιστή να μαθαίνει και να βελτιώνει συνεχώς την απόδοσή του, καθιστώντας τον έναν δυναμικό αντίπαλο στο παιχνίδι.


### Κύριες Παράμετροι στη Στρατηγική του Υπολογιστή

1. **Actions:**
   - **Τιμές:** `['R', 'P', 'S']` που αντιπροσωπεύουν Rock, Paper και Scissors.
   - **Σκοπός:** Αυτές οι ενέργειες είναι οι πιθανές κινήσεις που μπορεί να κάνει ο υπολογιστής.

2. **Gamma (`self.gamma`):**
   - **Τιμή:** `0.95`
   - **Σκοπός:** Ο συντελεστής έκπτωσης για τις μελλοντικές ανταμοιβές στον αλγόριθμο Q-learning. Ισορροπεί τη σημασία των άμεσων ανταμοιβών έναντι των μελλοντικών ανταμοιβών.
   - **Παραμετροποίηση:** Δοκιμάστε τιμές μεταξύ `0.8` και `0.99` για να δείτε πώς αλλάζει η απόδοση του πράκτορα. Οι υψηλότερες τιμές κάνουν τον πράκτορα να λαμβάνει περισσότερο υπόψη τις μακροπρόθεσμες ανταμοιβές.

3. **Epsilon (`self.epsilon`):**
   - **Αρχική Τιμή:** `0.5`
   - **Σκοπός:** Ο ρυθμός εξερεύνησης για τη στρατηγική epsilon-greedy. Οι υψηλότερες τιμές ενθαρρύνουν την εξερεύνηση νέων ενεργειών, ενώ οι χαμηλότερες τιμές ευνοούν την εκμετάλλευση γνωστών ενεργειών.
   - **Μείωση:** `0.995`
   - **Ελάχιστη Τιμή:** `0.05`
   - **Προσαρμογή:** Ρυθμίζεται δυναμικά με βάση τον δείκτη απόδοσης.
   - **Παραμετροποίηση:** Ρυθμίστε την αρχική τιμή (`0.1` έως `1.0`) και τον ρυθμό μείωσης (`0.9` έως `0.999`). Οι χαμηλότερες αρχικές τιμές ή οι βραδύτεροι ρυθμοί μείωσης μπορεί να κάνουν τον πράκτορα να εκμεταλλεύεται γνωστές στρατηγικές νωρίτερα.

4. **Learning Rate (`self.learning_rate`):**
   - **Τιμή:** `0.001`
   - **Σκοπός:** Το μέγεθος βήματος για την ενημέρωση των βαρών του νευρωνικού δικτύου κατά τη διάρκεια της εκπαίδευσης.
   - **Παραμετροποίηση:** Δοκιμάστε τιμές μεταξύ `0.0001` και `0.01`. Οι υψηλότεροι ρυθμοί μάθησης μπορούν να επιταχύνουν την εκμάθηση αλλά μπορεί να προκαλέσουν αστάθεια.

5. **Memory (`self.memory`):**
   - **Χωρητικότητα:** `10000`
   - **Σκοπός:** Αποθηκεύει παρελθούσες εμπειρίες (κατάσταση, δράση, ανταμοιβή, επόμενη κατάσταση, τέλος) για την εκπαίδευση του νευρωνικού δικτύου χρησιμοποιώντας αναπαραγωγή εμπειρίας.
   - **Παραμετροποίηση:** Αυξήστε τη χωρητικότητα σε `20000` ή μειώστε την σε `5000` για να δείτε πώς η χωρητικότητα της μνήμης επηρεάζει τη διαδικασία εκπαίδευσης.

6. **Dropout Rate (`self.dropout`):**
   - **Τιμή:** `0.3`
   - **Σκοπός:** Μια τεχνική κανονικοποίησης που χρησιμοποιείται στο μοντέλο DQN για την αποτροπή της υπερεκμάθησης.
   - **Παραμετροποίηση:** Ρυθμίστε μεταξύ `0.1` και `0.5`. Οι χαμηλότερες τιμές μπορεί να βελτιώσουν την εκμάθηση με τον κίνδυνο υπερεκμάθησης, ενώ οι υψηλότερες τιμές παρέχουν περισσότερη κανονικοποίηση.

7. **LSTM Parameters:**
   - **Hidden Dimension (`hidden_dim`):** `60`
   - **Number of Layers (`num_layers`):** `2`
   - **Σκοπός:** Οι παράμετροι του δικτύου LSTM καθορίζουν την πολυπλοκότητα και το βάθος του μοντέλου ακολουθίας που χρησιμοποιείται για την ανίχνευση προτύπων στις κινήσεις του παίκτη.
   - **Παραμετροποίηση:** Αυξήστε τη κρυφή διάσταση σε `100` ή προσθέστε περισσότερα στρώματα για να καταγράψετε πιο πολύπλοκα πρότυπα. Να είστε προσεκτικοί για την υπερεκμάθηση και το αυξημένο υπολογιστικό κόστος.

8. **Streak Counters:**
   - **Win Streak (`self.win_streak`):**
   - **Lose Streak (`self.lose_streak`):**
   - **Tie Streak (`self.tie_streak`):**
   - **Σκοπός:** Παρακολουθεί τα διαδοχικά αποτελέσματα για να προσαρμόζει δυναμικά τη στρατηγική και τον ρυθμό εξερεύνησης.
   - **Παραμετροποίηση:** Τροποποιήστε τα κατώφλια για την προσαρμογή του epsilon. Για παράδειγμα, αλλάξτε το κατώφλι νικηφόρου σερί σε `3` ή το κατώφλι ηττημένου σερί σε `2` για να κάνετε τον πράκτορα πιο ευαίσθητο στα σερί.

9. **Cluster Parameters:**
   - **Number of Clusters (`n_clusters`):** `5`
   - **Σκοπός:** Χρησιμοποιείται στην ομαδοποίηση KMeans για την ομαδοποίηση στρατηγικών αντιπάλων βάσει ιστορικών δεδομένων παιχνιδιού.
   - **Παραμετροποίηση:** Δοκιμάστε `3` έως `10` clusters για να δείτε πώς οι διαφορετικές ομαδοποιήσεις επηρεάζουν την ικανότητα του πράκτορα να ταξινομεί και να προσαρμόζεται σε διαφορετικές στρατηγικές αντιπάλων.

10. **Historical Data Processing:**
    - **Recent Length (`recent_length`):** `30`
    - **Σκοπός:** Ο αριθμός των πρόσφατων κινήσεων που λαμβάνονται υπόψη για ανάλυση συχνοτήτων και αναπαράσταση κατάστασης.
    - **Παραμετροποίηση:** Ρυθμίστε αυτό το μήκος σε `20` ή `50` για να δείτε πώς η διάρκεια της πρόσφατης ιστορίας που λαμβάνεται υπόψη επηρεάζει τις αποφάσεις του πράκτορα.

11. **State Size (`self.state_size`):**
    - **Τιμή:** `20` (αρχικό μέγεθος κατάστασης συν επιπλέον χαρακτηριστικά)
    - **Σκοπός:** Ορίζει τη διάσταση εισόδου για το νευρωνικό δίκτυο, συμπεριλαμβανομένων των συχνοτήτων κινήσεων, των πρόσφατων μετρήσεων κινήσεων και των σερί.
    - **Παραμετροποίηση:** Επεκτείνετε την αναπαράσταση της κατάστασης με επιπλέον χαρακτηριστικά όπως η ώρα της ημέρας ή τα ιστορικά ποσοστά νίκης του αντιπάλου.

### Μηχανισμοί Στρατηγικής

1. **Epsilon-Greedy Strategy:**
   - Επιλέγει τυχαίες ενέργειες με πιθανότητα epsilon για να εξασφαλίσει την εξερεύνηση.
   - Καθώς το epsilon μειώνεται, η στρατηγική βασίζεται περισσότερο στις προβλέψεις του Q-network (εκμετάλλευση).
   - **Παραμετροποίηση:** Ρυθμίστε την αρχική τιμή και τον ρυθμό μείωσης του epsilon για να βρείτε την ισορροπία μεταξύ εξερεύνησης και εκμετάλλευσης που ταιριάζει καλύτερα στο παιχνίδι σας.

2. **Q-Network and Target Network:**
   - **DQNetwork:** Το κύριο Q-network που εκπαιδεύεται για να προβλέπει Q-values.
   - **Target Network:** Ένα αντίγραφο του κύριου δικτύου, που ενημερώνεται περιοδικά για να σταθεροποιεί την εκπαίδευση.
   - **Παραμετροποίηση:** Ρυθμίστε τη συχνότητα ενημέρωσης του target network για να δείτε πώς επηρεάζει τη σταθερότητα της εκπαίδευσης.

3. **Experience Replay:**
   - Αποθηκεύει εμπειρίες και δειγματοληπτεί παρτίδες για την εκπαίδευση του Q-network.
   - **Batch Size:** Συνήθως `64`.
   - **Παραμετροποίηση:** Ρυθμίστε το μέγεθος παρτίδας μεταξύ `32` και `128` για να δείτε πώς επηρεάζει τη σταθερότητα και την απόδοση της εκπαίδευσης.

4. **Pattern Detection with LSTM:**
   - Χρησιμοποιεί LSTM για την ανίχνευση προτύπων στις κινήσεις του παίκτη σε μια ακολουθία.
   - Προβλέπει την επόμενη κίνηση του παίκτη βάσει ιστορικών ακολουθιών.
   - **Παραμετροποίηση:** Δοκιμάστε διαφορετικές διαστάσεις κρυφών στρώσεων και αριθμούς στρώσεων στο LSTM για να βρείτε τη βέλτιστη ρύθμιση για την ανίχνευση προτύπων.

5. **Adaptive Strategy:**
   - Ταξινομεί τη συμπεριφορά του αντιπάλου ως επιθετική, αμυντική ή τυχαία βάσει της συχνότητας των κινήσεων.
   - Προσαρμόζει τη στρατηγική δυναμικά βάσει της ταξινόμησης και των πρόσφατων δεικτών απόδοσης.
   - **Παραμετροποίηση:** Προσαρμόστε τα κατώφλια για την ταξινόμηση της συμπεριφοράς του αντιπάλου για να δείτε πώς επηρεάζει την προσαρμοστικότητα της στρατηγικής.

6. **Dynamic Learning Rate Adjustment:**
   - Αυξάνει τον ρυθμό μάθησης αν ο δείκτης απόδοσης (ποσοστό νικών) είναι υψηλός.
   - Μειώνει τον ρυθμό μάθησης αν ο δείκτης απόδοσης είναι χαμηλός.
   - **Παραμετροποίηση:** Ρυθμίστε τα ποσοστά αύξησης και μείωσης του ρυθμού μάθησης για να βρείτε την καλύτερη ρύθμιση για τη δυναμική προσαρμογή.

7. **Real-Time Learning:**
   - Ενημερώνει συνεχώς το Q-network με νέες εμπειρίες κατά τη διάρκεια του παιχνιδιού.
   - Προσαρμόζει το epsilon και τον ρυθμό μάθησης βάσει της τρέχουσας απόδοσης.
   - **Παραμετροποίηση:** Δοκιμάστε διαφορετικά ποσοστά ενημέρωσης και μεγέθη παρτίδας για την πραγματική εκμάθηση για να δείτε πώς επηρεάζουν την απόδοση του πράκτορα.

8. **Pre-training from Historical Data:**
   - Χρησιμοποιεί ιστορικά δεδομένα παιχνιδιού για την προεκπαίδευση του Q-network.
   - Βοηθά τον πράκτορα να ξεκινήσει με μια πιο ενημερωμένη στρατηγική βάσει παλαιότερων παιχνιδιών.
   - **Παραμετροποίηση:** Δοκιμάστε διαφορετικά μεγέθη και είδη ιστορικών δεδομένων για την προεκπαίδευση για να δείτε πώς επηρεάζουν την αρχική απόδοση του πράκτορα.

9. **Cluster Analysis:**
   - Χρησιμοποιεί KMeans για την αναγνώριση κοινών προτύπων στη στρατηγική του αντιπάλου.
   - Προσαρμόζει την προσέγγιση του πράκτορα βάσει του ανιχνευθέντος cluster.
   - **Παραμετροποίηση:** Δοκιμάστε διαφορετικούς αριθμούς clusters και παραμέτρους KMeans για να βρείτε την καλύτερη ομαδοποίηση στρατηγικών αντιπάλων.

### Περίληψη

Η στρατηγική του υπολογιστή σε αυτό το παιχνίδι Rock-Paper-Scissors έχει σχεδιαστεί για να είναι ιδιαίτερα προσαρμόσιμη και ανθεκτική. Συνδυάζει ενισχυτική μάθηση (Deep Q-Learning) με μοντέλα ακολουθίας (LSTM) και ομαδοποίηση (KMeans) για να προβλέψει και να αντιμετωπίσει τις κινήσεις του ανθρώπινου παίκτη αποτελεσματικά. Οι παράμετροι που ελέγχουν την εξερεύνηση, τους ρυθμούς μάθησης, τη μνήμη και την ανίχνευση προτύπων ρυθμίζονται δυναμικά για τη βελτίωση της απόδοσης με την πάροδο του χρόνου. Πειραματιζόμενοι με διαφορετικές τιμές για αυτές τις παραμέτρους, μπορείτε να βελτιστοποιήσετε και να βελτιώσετε την απόδοση του πράκτορα.

